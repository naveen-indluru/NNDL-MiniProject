{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r0QwnlWwGyZu"
   },
   "outputs": [],
   "source": [
    "pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "thxbrM7SeUtd"
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z-OA6p-r_rKp"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, STL10, MNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from torchinfo import summary\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FCyOFud4Gydf"
   },
   "outputs": [],
   "source": [
    "# mrl huggingfac dataset\n",
    "class HuggingfaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform, image_key=\"image\", label_key=\"label\"):\n",
    "        super(HuggingfaceDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.image_key = image_key\n",
    "        self.label_key = label_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx][self.image_key], int(\n",
    "            self.data[idx][self.label_key])\n",
    "\n",
    "        image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mfPJnCcLGzhf"
   },
   "outputs": [],
   "source": [
    "# mrl customdataset\n",
    "\n",
    "def tiny_imagenet(transform, split=\"train\"):\n",
    "    data = load_dataset('Maysee/tiny-imagenet', split=split)\n",
    "    return HuggingfaceDataset(data, transform)\n",
    "\n",
    "\n",
    "def food101(transform, split=\"train\"):\n",
    "    data = load_dataset('food101', split=split)\n",
    "    return HuggingfaceDataset(data, transform)\n",
    "\n",
    "\n",
    "def imagenet1k(transform, split=\"train\"):\n",
    "    data = load_dataset(\"imagenet-1k\", split=split)\n",
    "    return HuggingfaceDataset(data, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5bN4C6ioGzj1"
   },
   "outputs": [],
   "source": [
    "# mrl encoders\n",
    "\n",
    "def _adapt_resnet_model(model):\n",
    "    \"\"\"\n",
    "    Modifies some layers to handle the smaller CIFAR images, following\n",
    "    the SimCLR paper. Specifically, replaces the first conv layer with\n",
    "    a smaller 3x3 kernel and 1x1 strides and removes the max pooling layer.\n",
    "    \"\"\"\n",
    "    conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    nn.init.kaiming_normal_(conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "    model.conv1 = conv1\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "def _prep_encoder(model):\n",
    "    modules = list(model.children())[:-1]\n",
    "    modules.append(nn.AdaptiveAvgPool2d(1))\n",
    "    modules.append(Squeeze())\n",
    "\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "def resnet18(modify_model=False):\n",
    "    resnet = models.resnet18(weights=None)\n",
    "    if modify_model:\n",
    "        resnet = _adapt_resnet_model(resnet)\n",
    "    return _prep_encoder(resnet)\n",
    "\n",
    "\n",
    "def resnet50(modify_model=False):\n",
    "    resnet = models.resnet50(weights=None)\n",
    "    if modify_model:\n",
    "        resnet = _adapt_resnet_model(resnet)\n",
    "    return _prep_encoder(resnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ab8PyTCBGzmV"
   },
   "outputs": [],
   "source": [
    "# mrl aug\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class ViewGenerator(object):\n",
    "\n",
    "    def __init__(self, base_transform, n_views=2):\n",
    "        self.base_transform = base_transform\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transform(x) for i in range(self.n_views)]\n",
    "\n",
    "\n",
    "def _grayscale_to_rgb(img):\n",
    "    if img.mode == \"L\" or img.mode != \"RGB\":\n",
    "        return img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def _round_up_to_odd(num):\n",
    "    return np.ceil(num) // 2 * 2 + 1\n",
    "\n",
    "\n",
    "def get_training_transforms(image_size):\n",
    "    color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(_grayscale_to_rgb),\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([color_jitter], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.Lambda(_grayscale_to_rgb),\n",
    "        transforms.GaussianBlur(_round_up_to_odd(int(image_size * 0.1))),\n",
    "        transforms.RandomSolarize(127, 0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_inference_transforms(image_size=(96, 96)):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.Lambda(_grayscale_to_rgb),\n",
    "        transforms.ToTensor()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m4LQe23cGzoy"
   },
   "outputs": [],
   "source": [
    "# mrl utils\n",
    "\n",
    "def get_dataset(args):\n",
    "    dataset_name, dataset_path = args.dataset_name, args.dataset_path\n",
    "    if dataset_name == \"cifar10\":\n",
    "        return CIFAR10(dataset_path,\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=ViewGenerator(get_training_transforms(32), 2))\n",
    "    elif dataset_name == \"stl10\":\n",
    "        return STL10(dataset_path,\n",
    "                     split='unlabeled',\n",
    "                     download=True,\n",
    "                     transform=ViewGenerator(get_training_transforms(96), 2))\n",
    "    elif dataset_name == \"mnist\":\n",
    "        return MNIST(dataset_path,\n",
    "                             train=True,\n",
    "                             download=True,\n",
    "                             transform=ViewGenerator(get_training_transforms(28), 2))\n",
    "    elif dataset_name == \"tiny_imagenet\":\n",
    "        return tiny_imagenet(transform=ViewGenerator(get_training_transforms(64), 2))\n",
    "    elif dataset_name == \"food101\":\n",
    "        return food101(transform=ViewGenerator(get_training_transforms(192), 2))\n",
    "    elif dataset_name == \"imagenet1k\":\n",
    "        return imagenet1k(transform=ViewGenerator(get_training_transforms(192), 2))\n",
    "\n",
    "    raise Exception(\"Invalid dataset name - options are [cifar10, stl10]\")\n",
    "\n",
    "def get_encoder(model_name, modify_model=False):\n",
    "    if model_name == \"resnet18\":\n",
    "        return resnet18(modify_model)\n",
    "    elif model_name == \"resnet50\":\n",
    "        return resnet50(modify_model)\n",
    "    raise Exception(\n",
    "        \"Invalid model name - options are [resnet18, resnet50]\")\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "@torch.inference_mode\n",
    "def get_feature_size(encoder):\n",
    "    \"\"\"Get the feature size from the encoder using a dummy input.\"\"\"\n",
    "    encoder.eval()\n",
    "    dummy_input = torch.randn(1, 3, 32, 32)\n",
    "    output = encoder(dummy_input)\n",
    "    return output.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QITCLCzaIgyO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression(embeddings, labels, embeddings_val, labels_val):\n",
    "    X_train, X_test = embeddings, embeddings_val\n",
    "    y_train, y_test = labels, labels_val\n",
    "\n",
    "    clf = LogisticRegression(max_iter=100)\n",
    "    clf = CalibratedClassifierCV(clf)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy STL10: \", acc)\n",
    "\n",
    "\n",
    "class STL10Eval:\n",
    "\n",
    "    def __init__(self, image_size=96):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        transform = get_inference_transforms(image_size=(image_size, image_size))\n",
    "        train_ds = torchvision.datasets.STL10(\"data/\",\n",
    "                                        split='train',\n",
    "                                        transform=transform,\n",
    "                                        download=True)\n",
    "        val_ds = torchvision.datasets.STL10(\"data/\",\n",
    "                                        split='test',\n",
    "                                        transform=transform,\n",
    "                                        download=True)\n",
    "\n",
    "\n",
    "        self.train_loader = DataLoader(train_ds,\n",
    "                                batch_size=64,\n",
    "                                num_workers=4)\n",
    "        self.val_loader = DataLoader(val_ds,\n",
    "                            batch_size=64,\n",
    "                            num_workers=4)\n",
    "\n",
    "    @torch.inference_mode\n",
    "    def evaluate(self, relic_model):\n",
    "        relic_model.eval()\n",
    "        model = relic_model.target_encoder[0]\n",
    "        with torch.no_grad():\n",
    "            embeddings, labels = self._get_image_embs_labels(model, self.train_loader)\n",
    "            embeddings_val, labels_val = self._get_image_embs_labels(model, self.val_loader)\n",
    "\n",
    "            logistic_regression(embeddings, labels, embeddings_val, labels_val)\n",
    "\n",
    "    def _get_image_embs_labels(self, model, dataloader):\n",
    "        embs, labels = [], []\n",
    "        for _, (images, targets) in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                images = images.to(self.device)\n",
    "                out = model(images)\n",
    "                features = out.cpu().detach().tolist()\n",
    "                embs.extend(features)\n",
    "                labels.extend(targets.cpu().detach().tolist())\n",
    "        return np.array(embs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QA3-5yNBGz53"
   },
   "outputs": [],
   "source": [
    "# mrl\n",
    "\n",
    "class MatryoshkaProjector(nn.Module):\n",
    "    def __init__(self, nesting_dims, out_dims, **kwargs):\n",
    "        super(MatryoshkaProjector, self).__init__()\n",
    "        self.nesting_dims = nesting_dims\n",
    "        \n",
    "        if nesting_dims:\n",
    "            self.proj_hidden = nn.Linear(nesting_dims[-1], nesting_dims[-1], **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Error in MatryoshkaProjector: 'nesting_dims' is empty. Please ensure 'nesting_dims' is populated correctly.\")\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.proj_linear = nn.Linear(nesting_dims[-1], out_dims, **kwargs)\n",
    "\n",
    "    def _apply_linear_layer(self, x, layer, nesting_dim):\n",
    "        logits = torch.matmul(x[:, :nesting_dim], layer.weight[:, :nesting_dim].t())\n",
    "        if layer.bias is not None:\n",
    "            logits += layer.bias\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        nesting_logits = []\n",
    "        for nesting_dim in self.nesting_dims:\n",
    "            logits = self._apply_linear_layer(x, self.proj_hidden, nesting_dim)\n",
    "            logits = self.relu(logits)\n",
    "            logits = self._apply_linear_layer(x, self.proj_linear, nesting_dim)\n",
    "            nesting_logits.append(logits)\n",
    "        return nesting_logits\n",
    "\n",
    "\n",
    "def relic_loss(x, x_prime, temp, alpha, max_tau=5.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x (torch.Tensor): Online projections [n, dim].\n",
    "    x_prime (torch.Tensor): Target projections of shape [n, dim].\n",
    "    temp (torch.Tensor): Learnable temperature parameter.\n",
    "    alpha (float): KL divergence (regularization term) weight.\n",
    "    \"\"\"\n",
    "    n = x.size(0)\n",
    "    x, x_prime = F.normalize(x, p=2, dim=-1), F.normalize(x_prime, p=2, dim=-1)\n",
    "    logits = torch.mm(x, x_prime.t()) * temp.exp().clamp(0, max_tau)\n",
    "\n",
    "    # Instance discrimination loss\n",
    "    labels = torch.arange(n).to(logits.device)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "    # KL divergence loss\n",
    "    p1 = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "    p2 = torch.nn.functional.softmax(logits, dim=0).t()\n",
    "    invariance_loss = torch.nn.functional.kl_div(p1, p2, reduction=\"batchmean\")\n",
    "\n",
    "    loss = loss + alpha * invariance_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ReLIC(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 proj_out_dim=64,\n",
    "                 nesting_dims=None,\n",
    "                 proj_in_dim=1,\n",
    "                 matryoshka_bias=False):\n",
    "        super(ReLIC, self).__init__()\n",
    "\n",
    "        # Determine proj_in_dim if not provided\n",
    "        if not proj_in_dim:\n",
    "            proj_in_dim = get_feature_size(encoder)\n",
    "            print(f\"Debug: Calculated proj_in_dim = {proj_in_dim}\")\n",
    "            if proj_in_dim <= 0:\n",
    "                print(\"Warning: proj_in_dim is invalid (<= 0). Setting default proj_in_dim = 64.\")\n",
    "                proj_in_dim = 64\n",
    "            print(f\"Calculated proj_in_dim: {proj_in_dim}\")\n",
    "\n",
    "        # Set proj_in_dim to a default if it’s unexpectedly low (for testing)\n",
    "        if proj_in_dim <= 0:\n",
    "            print(\"Warning: proj_in_dim is zero or negative. Setting default proj_in_dim=64 for testing.\")\n",
    "            proj_in_dim = 64\n",
    "\n",
    "        # Populate nesting_dims if not provided or if empty\n",
    "        if not nesting_dims:\n",
    "            nesting_dims = [2 ** i for i in range(3, int(math.log2(proj_in_dim)) + 1)]\n",
    "            if not nesting_dims:\n",
    "                print(\"Warning: 'nesting_dims' is empty after calculation. Setting default nesting_dims = [8, 16, 32, 64].\")\n",
    "                nesting_dims = [8, 16, 32, 64]\n",
    "            if not nesting_dims:\n",
    "                raise ValueError(\"Error in ReLIC: Calculated 'nesting_dims' is empty, check 'proj_in_dim'.\")\n",
    "\n",
    "        print(f\"Final nesting_dims: {nesting_dims}\")  # Debug print to verify nesting_dims\n",
    "\n",
    "        # Initialize MatryoshkaProjector with verified nesting_dims\n",
    "        proj = MatryoshkaProjector(nesting_dims, proj_out_dim, bias=matryoshka_bias)\n",
    "\n",
    "        self.online_encoder = torch.nn.Sequential(encoder, proj)\n",
    "        self.target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        self.target_encoder.requires_grad_(False)\n",
    "        self.t_prime = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_features(self, img):\n",
    "        with torch.no_grad():\n",
    "            return self.target_encoder[0](img)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        o1, o2 = self.online_encoder(x1), self.online_encoder(x2)\n",
    "        with torch.no_grad():\n",
    "            t1, t2 = self.target_encoder(x1), self.target_encoder(x2)\n",
    "        t1 = [t_.detach() for t_ in t1]\n",
    "        t2 = [t_.detach() for t_ in t2]\n",
    "        return o1, o2, t1, t2\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_target_pred(self, x):\n",
    "        with torch.no_grad():\n",
    "            t = self.target_encoder(x)\n",
    "        t = [t_.detach() for t_ in t]\n",
    "        return t\n",
    "\n",
    "    def get_online_pred(self, x):\n",
    "        return self.online_encoder(x)\n",
    "\n",
    "    def update_params(self, gamma):\n",
    "        with torch.no_grad():\n",
    "            valid_types = [torch.float, torch.float16]\n",
    "            for o_param, t_param in self._get_params():\n",
    "                if o_param.dtype in valid_types and t_param.dtype in valid_types:\n",
    "                    t_param.data.lerp_(o_param.data, 1. - gamma)\n",
    "\n",
    "            for o_buffer, t_buffer in self._get_buffers():\n",
    "                if o_buffer.dtype in valid_types and t_buffer.dtype in valid_types:\n",
    "                    t_buffer.data.lerp_(o_buffer.data, 1. - gamma)\n",
    "\n",
    "    def copy_params(self):\n",
    "        for o_param, t_param in self._get_params():\n",
    "            t_param.data.copy_(o_param)\n",
    "\n",
    "        for o_buffer, t_buffer in self._get_buffers():\n",
    "            t_buffer.data.copy_(o_buffer)\n",
    "\n",
    "    def save_encoder(self, path):\n",
    "        torch.save(self.target_encoder[0].state_dict(), path)\n",
    "\n",
    "    def _get_params(self):\n",
    "        return zip(self.online_encoder.parameters(),\n",
    "                   self.target_encoder.parameters())\n",
    "\n",
    "    def _get_buffers(self):\n",
    "        return zip(self.online_encoder.buffers(),\n",
    "                   self.target_encoder.buffers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dA8ZekCcJSej"
   },
   "outputs": [],
   "source": [
    "# pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dpDISX_VGz8L"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "SEED = 42\n",
    "MAX_TAU = 5.0\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# cosine EMA schedule (increase from tau_base to one) as defined in https://arxiv.org/abs/2010.07922\n",
    "# k -> current training step, K -> maximum number of training steps\n",
    "def update_gamma(k, K, tau_base):\n",
    "    k = torch.tensor(k, dtype=torch.float32)\n",
    "    K = torch.tensor(K, dtype=torch.float32)\n",
    "\n",
    "    tau = 1 - (1 - tau_base) * (torch.cos(torch.pi * k / K) + 1) / 2\n",
    "    return tau.item()\n",
    "\n",
    "\n",
    "def train_relic(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    modify_model = True if \"cifar\" in args.dataset_name else False\n",
    "    encoder = get_encoder(args.encoder_model_name, modify_model)\n",
    "    relic_model = ReLIC(encoder,\n",
    "                        proj_out_dim=args.proj_out_dim)\n",
    "\n",
    "    if args.ckpt_path:\n",
    "        model_state = torch.load(args.ckpt_path)\n",
    "        relic_model.load_state_dict(model_state)\n",
    "    relic_model = relic_model.to(device)\n",
    "\n",
    "    # summary(relic_model, input_size=[(1, 3, 32, 32), (1, 3, 32, 32)])\n",
    "\n",
    "    params = list(relic_model.online_encoder.parameters()) + [relic_model.t_prime]\n",
    "    optimizer = torch.optim.Adam(params,\n",
    "                                 lr=args.learning_rate,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    ds = get_dataset(args)\n",
    "    num_workers_upd = max(0, args.num_workers)\n",
    "    train_loader = DataLoader(ds,\n",
    "                              batch_size=args.batch_size,\n",
    "                              num_workers=num_workers_upd,\n",
    "                              drop_last=True,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True)\n",
    "\n",
    "    scaler = GradScaler(enabled=args.fp16_precision)\n",
    "\n",
    "    stl10_eval = STL10Eval()\n",
    "    total_num_steps = (len(train_loader) *\n",
    "                       (args.num_epochs + 2)) - args.update_gamma_after_step\n",
    "    gamma = args.gamma\n",
    "    global_step = 0\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(args.num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader,\n",
    "                            desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
    "\n",
    "        for step, (images, _) in enumerate(progress_bar):\n",
    "            x1, x2 = images\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "\n",
    "            with autocast(enabled=args.fp16_precision):\n",
    "                o1, o2, t1, t2 = relic_model(x1, x2)\n",
    "                losses1 = [relic_loss(o,t,relic_model.t_prime,\n",
    "                                      args.alpha, max_tau=MAX_TAU)\n",
    "                                    for o, t in list(zip(o1, t2))]\n",
    "                loss1 = torch.stack(losses1).sum()\n",
    "                losses2 = [relic_loss(o,t,relic_model.t_prime,\n",
    "                                      args.alpha, max_tau=MAX_TAU)\n",
    "                                    for o, t in list(zip(o2, t1))]\n",
    "                loss2 = torch.stack(losses2).sum()\n",
    "                loss = (loss1 + loss2) / 2\n",
    "\n",
    "            # wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if global_step > args.update_gamma_after_step and global_step % args.update_gamma_every_n_steps == 0:\n",
    "                relic_model.update_params(gamma)\n",
    "                gamma = update_gamma(global_step, total_num_steps, args.gamma)\n",
    "\n",
    "            if global_step <= args.update_gamma_after_step:\n",
    "                relic_model.copy_params()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            avg_loss = total_loss / (global_step + 1)\n",
    "            ep_loss = epoch_loss / (step + 1)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1}/{args.num_epochs} | \"\n",
    "                f\"Step {global_step+1} | \"\n",
    "                f\"Epoch Loss: {ep_loss:.4f} |\"\n",
    "                f\"Total Loss: {avg_loss:.4f} |\"\n",
    "                f\"Gamma: {gamma:.6f} |\"\n",
    "                f\"Alpha: {args.alpha:.3f} |\"\n",
    "                f\"Temp: {relic_model.t_prime.exp().item():.3f} |\"\n",
    "                f\"Lr: {current_lr:.6f}\")\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.log_every_n_steps == 0:\n",
    "                with torch.no_grad():\n",
    "                    x, x_prime = o1[-1], t2[-1]\n",
    "                    x, x_prime = F.normalize(x, p=2, dim=-1), F.normalize(x_prime, p=2, dim=-1)\n",
    "                    logits = torch.mm(x, x_prime.t()) * relic_model.t_prime.exp().clamp(0, MAX_TAU)\n",
    "                labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "                top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "                accuracy1 = top1[0].item()\n",
    "                print(\"#\" * 100)\n",
    "                print('acc/top1 logits1', top1[0].item())\n",
    "                print('acc/top5 logits1', top5[0].item())\n",
    "                print(\"#\" * 100)\n",
    "                wandb.log({\"accuracy\": accuracy1})\n",
    "\n",
    "                torch.save(relic_model.state_dict(),\n",
    "                           \"relic_model.pth\")\n",
    "                relic_model.save_encoder(\"encoder.pth\")\n",
    "\n",
    "            if global_step % (args.log_every_n_steps * 5) == 0:\n",
    "                stl10_eval.evaluate(relic_model)\n",
    "                print(\"!\" * 100)\n",
    "\n",
    "            wandb.log({\"loss\": loss.item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYH-J250G0BP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find NNDL_Mini_Project_MRL.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaveen-indluru\u001b[0m (\u001b[33mnaveen-indluru-university-of-central-missouri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kbzwfo0p\n",
      "Sweep URL: https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/sweeps/kbzwfo0p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8b8a3s8p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_name: cifar10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_model_name: resnet18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find NNDL_Mini_Project_MRL.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Fall2024\\NeuralNetworkDeepLearning\\Project\\MiniProject\\NNDL_MRL_MiniProject\\wandb\\run-20241028_150209-8b8a3s8p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/runs/8b8a3s8p' target=\"_blank\">decent-sweep-1</a></strong> to <a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/sweeps/kbzwfo0p' target=\"_blank\">https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/sweeps/kbzwfo0p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project' target=\"_blank\">https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/sweeps/kbzwfo0p' target=\"_blank\">https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/sweeps/kbzwfo0p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/runs/8b8a3s8p' target=\"_blank\">https://wandb.ai/naveen-indluru-university-of-central-missouri/NNDL_Mini_Project/runs/8b8a3s8p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'nesting_dims' is empty after calculation. Setting default nesting_dims = [8, 16, 32, 64].\n",
      "Final nesting_dims: [8, 16, 32, 64]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# # run_training\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset_path = './data'\n",
    "        # choices=['stl10', 'cifar10', \"tiny_imagenet\", \"food101\", \"imagenet1k\"])\n",
    "        self.dataset_name = 'cifar10'\n",
    "        # choices=['resnet18', 'resnet50']\n",
    "        self.encoder_model_name = 'resnet18'\n",
    "        self.save_model_dir = './models'\n",
    "        self.num_epochs = 2\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 1e-2\n",
    "        self.weight_decay = 1e-4\n",
    "        self.fp16_precision = False  # Assuming default is not using 16-bit precision\n",
    "        self.proj_out_dim = 64\n",
    "        self.proj_hidden_dim = 512\n",
    "        self.log_every_n_steps = 400\n",
    "        self.gamma = 0.995\n",
    "        self.alpha = 0.5\n",
    "        self.update_gamma_after_step = 1\n",
    "        self.update_gamma_every_n_steps = 1\n",
    "        self.ckpt_path = None\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name': 'MRLsweep',  # Name of the sweep\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'dataset_name': {\n",
    "            'values': ['cifar10']\n",
    "            # 'values': ['stl10', 'cifar10', 'tiny_imagenet', 'food101', 'imagenet1k', 'mnist']\n",
    "        },\n",
    "        'encoder_model_name': {\n",
    "            'values': ['resnet18', 'resnet50']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-2, 1e-3, 1e-4]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-1, 1e-2, 1e-3]\n",
    "        },\n",
    "        'alpha': {\n",
    "            'values': [0.1, 0.5, 1.0]\n",
    "        },\n",
    "        'num_workers': {\n",
    "            'values': [0, 2, 4, 8]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"NNDL_Mini_Project_MRL.ipynb\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"NNDL_Mini_Project_MRL.ipynb\"\n",
    "wandb.login()\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"NNDL_Mini_Project\")\n",
    "\n",
    "def wrapper_train():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        args.dataset_name = config.dataset_name\n",
    "        args.encoder_model_name = config.encoder_model_name\n",
    "        args.batch_size = config.batch_size\n",
    "        args.learning_rate = config.learning_rate\n",
    "        args.weight_decay = config.weight_decay\n",
    "        args.alpha = config.alpha\n",
    "        args.num_workers = config.num_workers\n",
    "\n",
    "        train_relic(args)\n",
    "\n",
    "def main():\n",
    "    wandb.agent(sweep_id, function=wrapper_train)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
